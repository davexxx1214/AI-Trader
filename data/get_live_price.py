"""
Live Price Fetcher - å®æ—¶ä»·æ ¼æ•°æ®è·å–æ¨¡å—

ä» AlphaVantage API è·å–æœ€æ–°çš„å°æ—¶çº§è‚¡ç¥¨æ•°æ®ï¼Œå¹¶å¢é‡æ›´æ–°åˆ°æœ¬åœ°æ–‡ä»¶ã€‚
è®¾è®¡ç”¨äºå®æ—¶äº¤æ˜“åœºæ™¯ï¼Œåªè·å–æœ€è¿‘çš„æ•°æ®è€Œä¸æ˜¯å…¨é‡å†å²æ•°æ®ã€‚
"""

import os
import json
import sys
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any

import requests
from dotenv import load_dotenv

load_dotenv()

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from tools.trading_calendar import get_eastern_now, is_market_hours, format_eastern_time

# AlphaVantage API é…ç½®
APIKEY = os.getenv("ALPHAADVANTAGE_API_KEY")
FUNCTION = "TIME_SERIES_INTRADAY"
INTERVAL = "60min"
BASE_DIR = Path(__file__).resolve().parent

# çº³æ–¯è¾¾å…‹ 100 æˆåˆ†è‚¡
all_nasdaq_100_symbols = [
    "NVDA", "MSFT", "AAPL", "GOOG", "GOOGL", "AMZN", "META", "AVGO", "TSLA", "NFLX",
    "PLTR", "COST", "ASML", "AMD", "CSCO", "AZN", "TMUS", "MU", "LIN", "PEP",
    "SHOP", "APP", "INTU", "AMAT", "LRCX", "PDD", "QCOM", "ARM", "INTC", "BKNG",
    "AMGN", "TXN", "ISRG", "GILD", "KLAC", "PANW", "ADBE", "HON", "CRWD", "CEG",
    "ADI", "ADP", "DASH", "CMCSA", "VRTX", "MELI", "SBUX", "CDNS", "ORLY", "SNPS",
    "MSTR", "MDLZ", "ABNB", "MRVL", "CTAS", "TRI", "MAR", "MNST", "CSX", "ADSK",
    "PYPL", "FTNT", "AEP", "WDAY", "REGN", "ROP", "NXPI", "DDOG", "AXON", "ROST",
    "IDXX", "EA", "PCAR", "FAST", "EXC", "TTWO", "XEL", "ZS", "PAYX", "WBD",
    "BKR", "CPRT", "CCEP", "FANG", "TEAM", "CHTR", "KDP", "MCHP", "GEHC", "VRSK",
    "CTSH", "CSGP", "KHC", "ODFL", "DXCM", "TTD", "ON", "BIIB", "LULU", "CDW", "GFS",
]


def fetch_latest_price(symbol: str) -> Optional[Dict[str, Any]]:
    """
    ä» AlphaVantage è·å–æŒ‡å®šè‚¡ç¥¨çš„æœ€æ–°å°æ—¶çº§æ•°æ®
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        
    Returns:
        API è¿”å›çš„ JSON æ•°æ®ï¼Œå¤±è´¥è¿”å› None
    """
    if not APIKEY:
        print(f"âŒ ALPHAADVANTAGE_API_KEY æœªè®¾ç½®")
        return None
    
    params = {
        "function": FUNCTION,
        "symbol": symbol,
        "interval": INTERVAL,
        "outputsize": "compact",  # åªè·å–æœ€è¿‘ 100 æ¡æ•°æ®
        "apikey": APIKEY,
        "entitlement": "delayed",
        "extended_hours": "false",
    }
    
    url = "https://www.alphavantage.co/query"
    
    try:
        response = requests.get(url, params=params, timeout=15)  # å‡å°‘è¶…æ—¶æ—¶é—´
        response.raise_for_status()
        data = response.json()
        
        # æ£€æŸ¥ API é”™è¯¯
        if data.get("Note"):
            print(f"âš ï¸ {symbol}: API é™åˆ¶ - {data['Note']}")
            return None
        if data.get("Information"):
            print(f"âš ï¸ {symbol}: {data['Information']}")
            return None
        if "Error Message" in data:
            print(f"âŒ {symbol}: {data['Error Message']}")
            return None
            
        return data
    
    except requests.exceptions.Timeout:
        print(f"âš ï¸ {symbol}: è¯·æ±‚è¶…æ—¶ï¼Œè·³è¿‡")
        return None
    except requests.exceptions.ConnectionError:
        print(f"âš ï¸ {symbol}: è¿æ¥å¤±è´¥ï¼Œè·³è¿‡")
        return None
    except requests.exceptions.RequestException as e:
        print(f"âŒ {symbol}: ç½‘ç»œè¯·æ±‚å¤±è´¥ - {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ {symbol}: JSON è§£æå¤±è´¥ - {e}")
        return None


def update_local_file(symbol: str, new_data: Dict[str, Any]) -> bool:
    """
    å°†æ–°æ•°æ®å¢é‡æ›´æ–°åˆ°æœ¬åœ° JSON æ–‡ä»¶
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        new_data: ä» API è·å–çš„æ–°æ•°æ®
        
    Returns:
        æ˜¯å¦æ›´æ–°æˆåŠŸ
    """
    file_path = BASE_DIR / f"daily_prices_{symbol}.json"
    
    try:
        # è·å–æ–°æ•°æ®çš„æ—¶é—´åºåˆ—
        new_ts = new_data.get("Time Series (60min)", {})
        if not new_ts:
            print(f"âš ï¸ {symbol}: æ— æ—¶é—´åºåˆ—æ•°æ®")
            return False
        
        # å¦‚æœæœ¬åœ°æ–‡ä»¶å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if file_path.exists():
            with file_path.open('r', encoding='utf-8') as f:
                old_data = json.load(f)
            
            old_ts = old_data.get("Time Series (60min)", {})
            # åˆå¹¶ï¼šæ–°æ•°æ®è¦†ç›–æ—§æ•°æ®
            merged_ts = {**old_ts, **new_ts}
            
            merged_data = new_data.copy()
            merged_data["Time Series (60min)"] = merged_ts
            
            # ä¿ç•™æ—§çš„ Meta Data å¦‚æœæ–°æ•°æ®æ²¡æœ‰
            if "Meta Data" not in merged_data and "Meta Data" in old_data:
                merged_data["Meta Data"] = old_data["Meta Data"]
        else:
            merged_data = new_data
        
        # å†™å…¥æ–‡ä»¶
        with file_path.open('w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=4)
        
        # ç»Ÿè®¡æ–°å¢çš„æ•°æ®ç‚¹
        new_count = len(new_ts)
        total_count = len(merged_data.get("Time Series (60min)", {}))
        print(f"âœ… {symbol}: è·å– {new_count} æ¡æ–°æ•°æ®ï¼Œæ€»è®¡ {total_count} æ¡")
        
        return True
        
    except Exception as e:
        print(f"âŒ {symbol}: æ›´æ–°æ–‡ä»¶å¤±è´¥ - {e}")
        return False


def run_merge_jsonl() -> bool:
    """
    è¿è¡Œ merge_jsonl.py åˆå¹¶æ‰€æœ‰æ•°æ®åˆ° merged.jsonl
    
    Returns:
        æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
    """
    merge_script = BASE_DIR / "merge_jsonl.py"
    
    if not merge_script.exists():
        print(f"âŒ merge_jsonl.py ä¸å­˜åœ¨")
        return False
    
    try:
        result = subprocess.run(
            [sys.executable, str(merge_script)],
            cwd=str(BASE_DIR),
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode != 0:
            print(f"âŒ merge_jsonl.py æ‰§è¡Œå¤±è´¥: {result.stderr}")
            return False
        
        print("âœ… merged.jsonl å·²æ›´æ–°")
        return True
        
    except subprocess.TimeoutExpired:
        print("âŒ merge_jsonl.py æ‰§è¡Œè¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ merge_jsonl.py æ‰§è¡Œå¼‚å¸¸: {e}")
        return False


def fetch_all_symbols(symbols: Optional[List[str]] = None, 
                      delay_between_requests: float = 0.8) -> Dict[str, bool]:
    """
    è·å–æ‰€æœ‰è‚¡ç¥¨çš„æœ€æ–°æ•°æ®
    
    Args:
        symbols: è¦è·å–çš„è‚¡ç¥¨åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºçº³æ–¯è¾¾å…‹ 100
        delay_between_requests: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å… API é™åˆ¶
        
    Returns:
        æ¯ä¸ªè‚¡ç¥¨çš„è·å–ç»“æœ
    """
    import time
    
    if symbols is None:
        symbols = all_nasdaq_100_symbols
    
    results = {}
    total = len(symbols)
    success_count = 0
    consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°
    
    print(f"ğŸ“¡ å¼€å§‹è·å– {total} åªè‚¡ç¥¨çš„å®æ—¶æ•°æ®...")
    print(f"â° å½“å‰ç¾ä¸œæ—¶é—´: {format_eastern_time()}")
    print(f"ğŸ’¡ æç¤º: å¦‚æœé‡åˆ° API é™åˆ¶ï¼Œä¼šè‡ªåŠ¨å¢åŠ ç­‰å¾…æ—¶é—´")
    
    for i, symbol in enumerate(symbols, 1):
        print(f"[{i}/{total}] è·å– {symbol}...", end=" ", flush=True)
        
        data = fetch_latest_price(symbol)
        if data:
            success = update_local_file(symbol, data)
            results[symbol] = success
            if success:
                success_count += 1
                consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
        else:
            results[symbol] = False
            consecutive_failures += 1
            
            # å¦‚æœè¿ç»­å¤±è´¥è¶…è¿‡ 3 æ¬¡ï¼Œå¯èƒ½æ˜¯ API é™åˆ¶ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´
            if consecutive_failures >= 3:
                wait_time = 60  # ç­‰å¾… 60 ç§’
                print(f"âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„ API é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
                time.sleep(wait_time)
                consecutive_failures = 0
        
        # è¯·æ±‚é—´éš”ï¼Œé¿å… API é™é€Ÿ
        if i < total:
            time.sleep(delay_between_requests)
    
    print(f"\nğŸ“Š è·å–å®Œæˆ: {success_count}/{total} æˆåŠŸ")
    
    return results


def fetch_and_merge(symbols: Optional[List[str]] = None) -> bool:
    """
    è·å–æ‰€æœ‰æ•°æ®å¹¶åˆå¹¶åˆ° merged.jsonl
    è¿™æ˜¯å®æ—¶äº¤æ˜“è°ƒåº¦å™¨çš„ä¸»å…¥å£
    
    Args:
        symbols: è¦è·å–çš„è‚¡ç¥¨åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºçº³æ–¯è¾¾å…‹ 100
        
    Returns:
        æ˜¯å¦å…¨éƒ¨æˆåŠŸ
    """
    print("=" * 60)
    print(f"ğŸš€ å¼€å§‹å®æ—¶æ•°æ®æ›´æ–° - {format_eastern_time()}")
    print("=" * 60)
    
    # æ£€æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´
    if not is_market_hours():
        print("âš ï¸ å½“å‰ä¸åœ¨ç¾è‚¡äº¤æ˜“æ—¶é—´å†…")
        # ä»ç„¶ç»§ç»­æ‰§è¡Œï¼Œå› ä¸ºå¯èƒ½éœ€è¦è·å–æ”¶ç›˜åçš„æœ€ç»ˆæ•°æ®
    
    # è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    results = fetch_all_symbols(symbols)
    
    # åˆå¹¶åˆ° merged.jsonl
    merge_success = run_merge_jsonl()
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for v in results.values() if v)
    total_count = len(results)
    
    print("=" * 60)
    print(f"ğŸ“ˆ æ•°æ®æ›´æ–°å®Œæˆ")
    print(f"   - è‚¡ç¥¨è·å–: {success_count}/{total_count}")
    print(f"   - æ•°æ®åˆå¹¶: {'âœ… æˆåŠŸ' if merge_success else 'âŒ å¤±è´¥'}")
    print("=" * 60)
    
    return success_count > 0 and merge_success


def fetch_single_symbol(symbol: str) -> bool:
    """
    è·å–å•åªè‚¡ç¥¨çš„æ•°æ®ï¼ˆç”¨äºæµ‹è¯•æˆ–å•ç‹¬æ›´æ–°ï¼‰
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    print(f"ğŸ“¡ è·å– {symbol} çš„å®æ—¶æ•°æ®...")
    
    data = fetch_latest_price(symbol)
    if not data:
        return False
    
    success = update_local_file(symbol, data)
    if success:
        run_merge_jsonl()
    
    return success


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è·å–ç¾è‚¡å®æ—¶ä»·æ ¼æ•°æ®")
    parser.add_argument("--symbol", "-s", help="è·å–å•åªè‚¡ç¥¨ï¼ˆé»˜è®¤è·å–å…¨éƒ¨çº³æ–¯è¾¾å…‹100ï¼‰")
    parser.add_argument("--no-merge", action="store_true", help="ä¸æ‰§è¡Œåˆå¹¶æ“ä½œ")
    
    args = parser.parse_args()
    
    if args.symbol:
        # è·å–å•åªè‚¡ç¥¨
        success = fetch_single_symbol(args.symbol.upper())
        sys.exit(0 if success else 1)
    else:
        # è·å–å…¨éƒ¨è‚¡ç¥¨
        results = fetch_all_symbols()
        
        if not args.no_merge:
            run_merge_jsonl()
        
        # è¿”å›é€€å‡ºç 
        success_count = sum(1 for v in results.values() if v)
        sys.exit(0 if success_count > 0 else 1)

